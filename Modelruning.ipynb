{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config import *\n",
    "from Dataloader import *\n",
    "from Feateng import *\n",
    "from Basic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dim, aggr, ni, act, bs, task, target, rand_cycle=None, save_model=False):\n",
    "    params = get_params(task) \n",
    "    params[\"props\"] = [target] \n",
    "    if params[\"rand_test\"]:\n",
    "        params[\"rand_cycle\"] = rand_cycle \n",
    "    print(params, flush=True)\n",
    "    random.seed(params[\"rand_seed\"])\n",
    "    torch.manual_seed(params[\"rand_seed\"])\n",
    "    np.random.seed(random.seed(params[\"rand_seed\"])) \n",
    "\n",
    "    dataset = MOFDataset(params) \n",
    "    dataset.n2v_embedding(params[\"adj_gen_state\"], params[\"n2v_emb_state\"])\n",
    "    dataset.assig_feature() \n",
    "    if params[\"y_scaler\"]:\n",
    "        dataset, y_scaler = y_scaling(dataset)\n",
    "    if params[\"use_struc\"]:\n",
    "        dataset, struc_scaler = struc_scaling(dataset) \n",
    "        if save_model:\n",
    "            joblib.dump(struc_scaler, \"\".join([params[\"output_path\"], \"struc_scaler_\", params[\"props\"][0], \".pkl\"]))\n",
    "    data_loader = dataset.data_load() \n",
    "    node, prop = [], [] \n",
    "    for i in range(len(data_loader)): \n",
    "        node.append(data_loader[i].atom_num)\n",
    "        prop.append(data_loader[i].y) \n",
    "    print(\"Largest graph with nodes of\", np.max(node), np.argmax(node), flush=True)\n",
    "    print(\"Average prop:\", np.average(prop), flush=True) \n",
    "    random.shuffle(data_loader) \n",
    "\n",
    "    data_size = int(len(data_loader) * 0.1) \n",
    "    test_dataset = data_loader[:data_size] \n",
    "    val_dataset = data_loader[data_size:2 * data_size] \n",
    "    train_dataset = data_loader[2 * data_size:] \n",
    "\n",
    "    print(\"Start training...\", flush=True) \n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=False) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=data_size, shuffle=False) \n",
    "    test_loader = DataLoader(test_dataset, batch_size=data_size, shuffle=False) \n",
    "    print(f\"MOF in training set: {len(train_dataset)}\", flush=True) \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    hyper = [ni, dim, aggr, act, bs] \n",
    "    print(f\"\\nCase: {'-'.join(map(str, hyper))}\", flush=True) \n",
    "    params[\"iden_num\"], params[\"node_num\"], params[\"topo_num\"], params[\"topo_pad\"] = \\\n",
    "        len(dataset.iden_c2i), len(dataset.node_c2i), len(dataset.topo_c2i), dataset.topo_pad \n",
    "    if params[\"use_chem\"]:\n",
    "        model = MOFNet(params, ni, dim, aggr, act)\n",
    "    else:\n",
    "        model = MOFNet_FNN(params, ni, dim, aggr, act)\n",
    "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad), flush=True) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"], weight_decay=1e-5) \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='min',\n",
    "                                                           factor=params[\"scheduler_factor\"],\n",
    "                                                           patience=params[\"scheduler_patience\"],\n",
    "                                                           min_lr=0.0001)\n",
    "    earlystop_er = EarlyStopping(patience=params[\"earlystop_er_patience\"]) \n",
    "\n",
    "    for i in range(params[\"epoch\"]): \n",
    "        if not earlystop_er.early_stop: \n",
    "            lr = scheduler.optimizer.param_groups[0]['lr'] \n",
    "            model.train() \n",
    "            loss_all = 0 \n",
    "            for data in train_loader: \n",
    "                data = data.to(device) \n",
    "                optimizer.zero_grad() \n",
    "                tmp_pred_data = model(data) \n",
    "                data.y = torch.tensor(data.y, dtype=torch.float32) \n",
    "                tmp_real_data = data.y[~torch.isnan(data.y)].view(-1, 1) \n",
    "                tmp_pred_data_mod = tmp_pred_data[~torch.isnan(data.y)].view(-1, 1) \n",
    "                loss = F.mse_loss(tmp_pred_data_mod, tmp_real_data) \n",
    "                loss.backward() \n",
    "                loss_all += loss.item() * data.num_graphs \n",
    "                optimizer.step() \n",
    "            loss = loss_all / len(train_loader.dataset) \n",
    "\n",
    "            def test(loader, set):\n",
    "                model.eval() \n",
    "                dfs, names, sets = [], [], [] \n",
    "                for data in loader: \n",
    "                    data.y = torch.tensor(data.y, dtype=torch.float32) \n",
    "                    df = pd.DataFrame(np.concatenate((data.y, model(data).detach().numpy()), axis=1),\n",
    "                                      columns=[\"exp\", \"pre\"]) \n",
    "                    dfs.append(df) \n",
    "                    names += data.name \n",
    "                    sets += [set] * len(data.name) \n",
    "                DF = pd.concat(dfs, ignore_index=True) \n",
    "                DF[\"name\"] = names \n",
    "                DF[\"set\"] = sets \n",
    "                exp, pre = DF[\"exp\"].values, DF[\"pre\"].values \n",
    "                r2 = r2_score(exp, pre) \n",
    "                mae = mean_absolute_error(exp, pre) \n",
    "                mse = mean_squared_error(exp, pre) \n",
    "\n",
    "                return r2, mae, mse, DF \n",
    "\n",
    "            r2_tr, mae_tr, mse_tr, df_tr = test(train_loader, \"training\") \n",
    "            r2_val, mae_val, mse_val, df_val = test(val_loader, \"validation\") \n",
    "            r2_te, mae_te, mse_te, df_te = test(test_loader, \"test\") \n",
    "\n",
    "            scheduler.step(mse_val) \n",
    "            earlystop_er(mse_val)  \n",
    "\n",
    "            print('Epoch: {:03d}, LR: {:7f}, Loss: {:.4f}, MSE: {:.4f}/{:.4f}/{:.4f},' \n",
    "                  'MAE: {:.4f}/{:.4f}/{:.4f}, R2: {:.4f}/{:.4f}/{:.4f}'\n",
    "                  .format(i + 1, lr, loss, mse_tr, mse_val, mse_te, mae_tr, mae_val, mae_te, \n",
    "                          r2_tr, r2_val, r2_te), flush=True) \n",
    "\n",
    "            if earlystop_er.update: \n",
    "                update = [i + 1, loss, mse_tr, mae_tr, r2_tr, mse_val, mae_val, r2_val,\n",
    "                          mse_te, mae_te, r2_te]\n",
    "                model_update = model\n",
    "                df_tr_update = df_tr\n",
    "                df_val_update = df_val\n",
    "                df_te_update = df_te\n",
    "                if save_model:\n",
    "                    torch.save(model_update, \"\".join(\n",
    "                        [params[\"output_path\"], \"_\".join(map(str, hyper)), \"_\", params[\"props\"][0], \"_model.pkl\"]))\n",
    "\n",
    "    return r2_te, mae_te, mse_te\n",
    "\n",
    "\n",
    "graph_convolutions = ['GINConv', 'GCNConv', 'AGNNConv', 'ClusterGCNConv', 'GATConv', \n",
    "                      'GraphConv', 'LEConv', 'MFConv', 'SAGEConv']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hidden_layer_widths = [8, 16, 24, 32]\n",
    "    activation_functions = ['tanh', 'elu', 'relu', 'sigmoid', 'softplus']\n",
    "    batch_sizes = [64, 128, 256]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for width in hidden_layer_widths:\n",
    "        for act_function in activation_functions:\n",
    "            for batch_size in batch_sizes:\n",
    "                for conv_index, conv_method in enumerate(graph_convolutions):\n",
    "                    r2, rmse, mae = train(width, \"mean\", conv_index, act_function, batch_size, \"C2\", \"Capacity\", save_model=False)\n",
    "                    results.append([width, act_function, batch_size, conv_method, r2, rmse, mae])\n",
    "\n",
    "    with open('training_results.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Hidden Layer Width', 'Activation Function', 'Batch Size', 'Graph Convolution', 'R2', 'RMSE', 'MAE'])\n",
    "        writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
