{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "globals().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '.../...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP f interpretion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open(\".../best_model.pkl\", 'rb'))\n",
    "df = pd.read_csv(\".../trainingdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ad = df.drop(['Capacity'],axis=1)\n",
    "# struct \n",
    "X_non_cat1 = X_ad.drop(['HPC_index', 'name', 'metal_smiles',  'organ_smiles', 'topo', 'GLD', 'GSA', 'VF', 'Tem', 'pH', 'Dosage', 'Concentration'],axis=1)\n",
    "# env\n",
    "X_non_cat2 = X_ad.drop(['HPC_index', 'name', 'metal_smiles', 'organ_smiles', 'topo',  'GLD', 'PLD', 'LCD', 'ρ', 'VSA', 'GSA', 'VF', 'AV'],axis=1)\n",
    "# framework1\n",
    "Metal1 = X_ad.loc[:,'metal_smiles']\n",
    "Frame = X_ad.loc[:,'topo']\n",
    "\n",
    "X_cat1 = pd.concat([Metal1],axis=1)\n",
    "X_cat3 = pd.concat([Frame],axis=1)\n",
    "# framework2\n",
    "Mfs1 = pd.read_csv('.../MF/...').loc[:, '0':] \n",
    "X_mf1 = Mfs1\n",
    "X_non_cat3 = X_mf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(X_cat1)\n",
    "X_encoded1 = enc.transform(X_cat1).toarray()\n",
    "feature_names1 = X_cat1.columns\n",
    "new_feature_names1 = enc.get_feature_names_out(feature_names1)\n",
    "X_encoded1 = pd.DataFrame(X_encoded1, columns= new_feature_names1)\n",
    "X_encoded1.columns = X_encoded1.columns.str.replace('metal_smiles_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(X_cat3)\n",
    "X_encoded3 = enc.transform(X_cat3).toarray()\n",
    "feature_names3 = X_cat3.columns\n",
    "new_feature_names3 = enc.get_feature_names_out(feature_names3)\n",
    "X_encoded3 = pd.DataFrame(X_encoded3, columns= new_feature_names3)\n",
    "X_encoded3.columns = X_encoded3.columns.str.replace('topo_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_non_cat1, X_non_cat2, X_non_cat3, X_encoded1, X_encoded3],axis=1)\n",
    "Y = df.loc[:,'Capacity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = 0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(best_model)\n",
    "X_sc = sc.transform(X)\n",
    "X_sc = pd.DataFrame(X_sc, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    '[Al]': 'S1', '[Al][O](Al])[Al]': 'S2', '[Ce]': 'S3', '[Co]': 'S4', \n",
    "    '[Co][OH]1[Co][OH]2[Co]1([OH2])([OH2])[OH2][Co]2': 'S5', \n",
    "    '[Cr][O]([Cr])[Cr]': 'S6', '[Cu][O]1[Cu][O]([Cu]1)[Cu]': 'S7', '[Eu]': 'S8', \n",
    "    '[Fe]': 'S9', '[Fe][O]([Fe])[Fe]': 'S10', '[In]': 'S11', '[La]': 'S12', \n",
    "    '[Ni]': 'S13', '[O][Ce]123([O])[O]4[Ce]56([O]3[Ce]37([O]2[Ce]28([O]1[Ce]14([O]6[Ce]([O]53)([O]21)([O]78)([O])[O])([O])[O])([O])[O])([O])[O])([O])[O]': 'S14', \n",
    "    '[O][Fe][O]([Fe][O])[Fe][O]': 'S15', '[O][Zr]123[O]4[Zr]56[O]3[Zr]37([O]2[Zr]28[O]1[Zr]14([O]6[Zr]([O]53)([O]21)([O]78)[O])[O])[O]': 'S16', \n",
    "    '[O]12[Ce]34[O]5[Ce]62[O]2[Ce]71[O]4[Ce]14[O]3[Ce]35[O]6[Ce]2([O]71)[O]43': 'S17', \n",
    "    '[O]12[Zr]34[O]5[Zr]62[O]2[Zr]71[O]4[Zr]14[O]3[Zr]35[O]6[Zr]2([O]71)[O]43': 'S18', \n",
    "    '[O]12[Zr]34[OH]5[Zr]62[OH]2[Zr]71[OH]4[Zr]14[O]3[Zr]35[O]6[Zr]2([O]71)[OH]43': 'S19', \n",
    "    '[OH2][Co]([OH2])([OH2])[OH2]': 'S20', '[OH2][Yb]': 'S21', '[Pb][OH][Pb]': 'S22', \n",
    "    '[Sn][O]([Sn])[Sn]': 'S23', '[Y]': 'S24', '[Zn]': 'S25', '[Zn][O]([Zn])([Zn])[Zn]': 'S26', \n",
    "    '[Zn][OH2][Zn]': 'S27', '[Zr]': 'S28', 'O1[Ti]23O[Ti]3(O2)O[Ti]23O[Ti]3(O2)O[Ti]23[Ti](O[Ti]45[Ti]1(O4)O5)(O2)O3': 'S29'\n",
    "}\n",
    "X_sc.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_array = shap_values.values\n",
    "shap_values_df = pd.DataFrame(shap_values_array, columns=X_sc.columns) #total\n",
    "#shap_values_df.to_csv(\"1-shap.csv\", index=False)\n",
    "mean_abs_shap_values = np.abs(shap_values.values).mean(axis=0)\n",
    "mean_abs_shap_df = pd.DataFrame(mean_abs_shap_values, index=X.columns, columns=['mean_abs_shap_value']) #absolute average \n",
    "mean_abs_shap_df .to_csv(\"....csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#b2e1b9', '#77cac5', '#42a6cb', '#1373b2', '#084384'] \n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "shap.summary_plot(shap_values, X, feature_names = X.columns, class_inds='original', cmap=cmap )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_show = X_encoded3.columns#X_encoded3, X_non_cat3\n",
    "\n",
    "\n",
    "feature_indices = [X.columns.get_loc(col) for col in features_to_show if col in X.columns]\n",
    "shap_values_filtered = shap_values[:, feature_indices]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "shap.summary_plot(shap_values_filtered, X[features_to_show], feature_names=features_to_show, cmap=cmap)\n",
    "\n",
    "#plt.savefig('shap2.pdf', format='pdf', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdp for numerical f interpretion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pickle.load(open(\".../best_model.pkl\", 'rb'))\n",
    "df = pd.read_csv(\".../trainingdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOF_construction = df.loc[:,'name':'topo']\n",
    "Structure_info = df.loc[:,'GLD':'AV']\n",
    "Absorb_env = df.loc[:,'Tem':'Concentration']\n",
    "Capacity = df.loc[:,'Capacity']\n",
    "Mfs1 = pd.read_csv(path2 + 'Morgan.csv').loc[:, '0':] \n",
    "\n",
    "Metal1 = pd.get_dummies(MOF_construction.loc[:,'metal_smiles'])\n",
    "Topo = pd.get_dummies(MOF_construction.loc[:,'topo'])\n",
    "Data = pd.concat([MOF_construction, Structure_info, Absorb_env, Mfs1, Metal1, Topo],axis=1) \n",
    "X = Data.drop(['name', 'metal_smiles',  'organ_smiles', 'topo', 'GLD', 'GSA', 'VF'],axis=1)\n",
    "Y = Capacity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "X_new_scaled = np.concatenate((X_train_scaled, X_test_scaled), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_rank = ['PLD', 'LCD', 'ρ', 'VSA', 'AV', 'Tem', 'pH', 'Dosage', 'Concentration']\n",
    "Top_rank_ind = [X.columns.get_loc(c) for c in Top_rank]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in zip(Top_rank, Top_rank_ind):\n",
    "    print(f\"Feature: {i}, Index: {idx}\")\n",
    "    results = partial_dependence(cb_run, X_train_scaled, [idx])\n",
    "    x_data = results['values'][0]\n",
    "    pdp = results['average'].ravel()\n",
    "\n",
    "    x_data_orig_scale = sc.inverse_transform(np.vstack([x_data if j == idx else np.zeros(len(x_data)) for j in range(X_train_scaled.shape[1])]).T)[:, idx]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 4))\n",
    "    ax1.plot(x_data_orig_scale, pdp, color='#1373b2', linewidth=3)\n",
    "    ax1.set_ylabel('Partial dependence', fontsize=20)#fontweight='bold',\n",
    "    ax1.set_xlabel(i, fontsize=20)\n",
    "\n",
    "    ax1.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    sns.rugplot(X_train.iloc[:, idx], color='#1373b2', ax=ax1)\n",
    "    sns.distplot(X_train.iloc[:, idx], kde=False, color='#b2e1b9', ax=ax2)\n",
    "    ax2.set_ylabel('Number of data', fontsize=20)\n",
    "    ax2.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "    plt.style.use('default')\n",
    "\n",
    "    for spine in ax1.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    for spine in ax2.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    plt.show()\n",
    " \n",
    "    #plt.savefig(f\"pdp_{i}.png\", format='png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "#print(plt.style.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "Xper = df.drop(['HPC_index','name', 'GLD', 'Capacity'],axis=1)\n",
    "corr = Xper.corr()\n",
    "pval = pd.DataFrame(index=corr.index, columns=corr.columns)\n",
    "\n",
    "for col in corr.columns:\n",
    "    for row in corr.index:\n",
    "        pval.loc[row, col] = pearsonr(Xper[row], Xper[col])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "ax = sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', linewidths=0.3, cmap='GnBu', annot_kws={\"size\": 13})\n",
    "\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i):  \n",
    "        p_value = pval.iloc[i, j]\n",
    "        if p_value < 0.05: \n",
    "            plt.text(j + 0.5, i + 0.2, '*', ha='center', va='center', color='black', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d-pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "features_to_plot = ['LCD', 'VSA', ('LCD', 'VSA')] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double\n",
    "line_kw = {\"color\": \"#1373b2\"} \n",
    "contour_kw = {\"cmap\": \"GnBu\"} \n",
    "\n",
    "_, ax = plt.subplots(ncols=3, figsize=(15, 4), constrained_layout=True)\n",
    "\n",
    "tic = time.time()\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    cb_run,\n",
    "    X_train_scaled_df,\n",
    "    features=features_to_plot,\n",
    "    kind='average',\n",
    "    ax=ax,\n",
    "    line_kw=line_kw,\n",
    "    contour_kw=contour_kw\n",
    ")\n",
    "print(f\"done in {time.time() - tic:.3f}s\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    locs = ax[i].get_xticks()  \n",
    "    if isinstance(feature, str):  \n",
    "        feature_idx = X_train.columns.get_loc(feature)  \n",
    "        temp = np.zeros((len(locs), X_train_scaled.shape[1]))  \n",
    "        temp[:, feature_idx] = locs \n",
    "        original_values = sc.inverse_transform(temp)[:, feature_idx] \n",
    "        ax[i].xaxis.set_major_locator(FixedLocator(locs))  \n",
    "        ax[i].xaxis.set_major_formatter(FixedFormatter([f'{val:.0f}' for val in original_values]))  #\n",
    "        ax[i].set_xlabel(feature, fontsize=15)\n",
    "\n",
    "    elif isinstance(feature, tuple):  \n",
    "        # For X-axis\n",
    "        feature_idx_x = X_train.columns.get_loc(feature[0]) \n",
    "        temp_x = np.zeros((len(locs), X_train_scaled.shape[1]))  \n",
    "        temp_x[:, feature_idx_x] = locs\n",
    "        original_values_x = sc.inverse_transform(temp_x)[:, feature_idx_x]\n",
    "        ax[i].xaxis.set_major_locator(FixedLocator(locs))\n",
    "        ax[i].xaxis.set_major_formatter(FixedFormatter([f'{val:.2f}' for val in original_values_x])) #\n",
    "        \n",
    "        # For Y-axis\n",
    "        locs_y = ax[i].get_yticks() \n",
    "        feature_idx_y = X_train.columns.get_loc(feature[1])  \n",
    "        temp_y = np.zeros((len(locs_y), X_train_scaled.shape[1])) \n",
    "        temp_y[:, feature_idx_y] = locs_y\n",
    "        original_values_y = sc.inverse_transform(temp_y)[:, feature_idx_y]\n",
    "        ax[i].yaxis.set_major_locator(FixedLocator(locs_y))\n",
    "        ax[i].yaxis.set_major_formatter(FixedFormatter([f'{val:.0f}' for val in original_values_y])) #\n",
    "\n",
    "        ax[i].set_xlabel(feature[0], fontsize=20)\n",
    "        ax[i].set_ylabel(feature[1], fontsize=20)\n",
    "    ax[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "\n",
    "ax[0].set_visible(False)\n",
    "ax[1].set_visible(False)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdp for non-numerical f interpretion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_run = pickle.load(open(\"/Users/linzuhong/学习文件/3-博/博三/文章/2-可解释机器学习MOFs筛选-2/data/Output/XGB/XGB1_model.pkl\", 'rb'))\n",
    "df = pd.read_csv(\"/Users/linzuhong/学习文件/3-博/博三/文章/2-可解释机器学习MOFs筛选-2/data/Absorb00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOF_construction = df.loc[:,'name':'topo']\n",
    "Structure_info = df.loc[:,'GLD':'AV']\n",
    "Absorb_env = df.loc[:,'Tem':'Concentration']\n",
    "Capacity = df.loc[:,'Capacity']\n",
    "Mfs1 = pd.read_csv('/Users/linzuhong/学习文件/3-博/博三/文章/2-可解释机器学习MOFs筛选/data/MF/Morgan.csv').loc[:, '0':] \n",
    "\n",
    "Metal1 = pd.get_dummies(MOF_construction.loc[:,'metal_smiles'])\n",
    "#Metal2 = pd.get_dummies(MOF_construction.loc[:,'metal_types'])\n",
    "Topo = pd.get_dummies(MOF_construction.loc[:,'topo'])\n",
    "Data = pd.concat([MOF_construction, Structure_info, Absorb_env, Mfs1, Metal1, Topo],axis=1) \n",
    "X = Data.drop(['name', 'metal_smiles', 'organ_smiles', 'topo', 'GLD', 'AV','GSA'],axis=1)\n",
    "Y = Capacity\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train_df.loc[:,'O1[Ti]23O[Ti]3(O2)O[Ti]23O[Ti]3(O2)O[Ti]23[Ti](O[Ti]45[Ti]1(O4)O5)(O2)O3':'[Zr]']\n",
    "rename_dict = {\n",
    "    'O1[Ti]23O[Ti]3(O2)O[Ti]23O[Ti]3(O2)O[Ti]23[Ti](O[Ti]45[Ti]1(O4)O5)(O2)O3': 'A', '[Al]': 'B', '[Al][O](Al])[Al]': 'C', '[Ce]': 'D', '[Co]': 'E', '[Co][OH]1[Co][OH]2[Co]1([OH2])([OH2])[OH2][Co]2': 'F',    \n",
    "    '[Cr][O]([Cr])[Cr]': 'G', '[Eu]': 'H', '[Fe]': 'I', '[Fe][O]([Fe])[Fe]': 'J', '[In]': 'K',\n",
    "    '[La]': 'L', '[Ni]': 'M', '[OH2][Co]([OH2])([OH2])[OH2]': 'N', '[OH2][Yb]': 'O', '[O]12[Ce]34[O]5[Ce]62[O]2[Ce]71[O]4[Ce]14[O]3[Ce]35[O]6[Ce]2([O]71)[O]43': 'P',    \n",
    "    '[O]12[Zr]34[OH]5[Zr]62[OH]2[Zr]71[OH]4[Zr]14[O]3[Zr]35[O]6[Zr]2([O]71)[OH]43': 'Q', '[O]12[Zr]34[O]5[Zr]62[O]2[Zr]71[O]4[Zr]14[O]3[Zr]35[O]6[Zr]2([O]71)[O]43': 'R',    \n",
    "    '[O][Ce]123([O])[O]4[Ce]56([O]3[Ce]37([O]2[Ce]28([O]1[Ce]14([O]6[Ce]([O]53)([O]21)([O]78)([O])[O])([O])[O])([O])[O])([O])[O])([O])[O]': 'S', '[O][Fe][O]([Fe][O])[Fe][O]': 'T',    \n",
    "    '[O][Fe][O]([Fe][O])[Fe][O]': 'U',  '[O][Zr]123[O]4[Zr]56[O]3[Zr]37([O]2[Zr]28[O]1[Zr]14([O]6[Zr]([O]53)([O]21)([O]78)[O])[O])[O]': 'V',  '[Pb][OH][Pb]': 'W',    \n",
    "    '[Sn][O]([Sn])[Sn]': 'X', '[Y]': 'Y', '[Zn]': 'Z', '[Zn][OH2][Zn]': 'a', '[Zr]': 'b'\n",
    "}\n",
    "X_train1 = X_train1.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.columns = X_train_df.columns.str.replace('metal_smiles_', '')\n",
    "#X_train_df.columns = X_train_df.columns.str.replace('metal_types_', '')\n",
    "X_train_df.columns = X_train_df.columns.str.replace('topo_', '')\n",
    "\n",
    "X_train1 = X_train_df.loc[:,'O1[Ti]23O[Ti]3(O2)O[Ti]23O[Ti]3(O2)O[Ti]23[Ti](O[Ti]45[Ti]1(O4)O5)(O2)O3':'[Zr]']\n",
    "X_train2 = X_train_df.loc[:,'acs':'sql']\n",
    "#X_train3 = X_train_df.loc[:,'Al':'Zr']\n",
    "\n",
    "features_of_interest_sets = {\n",
    "    'Set1': X_train1.columns.tolist(),\n",
    "    'Set2': X_train2.columns.tolist()}\n",
    "    #'Set3': X_train3.columns.tolist()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_results_all = {}\n",
    "for feature in X_train_df.columns:\n",
    "    pdp_res = partial_dependence(cb_run, X_train_df, features=[feature])\n",
    "    pdp_results_all[feature] = pdp_res\n",
    "\n",
    "\n",
    "for set_name, features in features_of_interest_sets.items():\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    y_pos = np.arange(len(features))\n",
    "    avg_pdp_values = [pdp_results_all[feature]['average'][0][1] for feature in features if feature in pdp_results_all]\n",
    "    \n",
    "    if avg_pdp_values:  \n",
    "        ax1.plot(y_pos, avg_pdp_values, marker='o', linestyle='-', color='#1373b2')\n",
    "        ax1.set_xticks(y_pos)\n",
    "        ax1.set_xticklabels(features, rotation=45, fontweight='bold', fontstyle='italic')\n",
    "        ax1.set_ylabel('Partial dependence')#fontweight='bold'\n",
    "        ax1.set_title(f'Partial dependence for {set_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No PDP data available for feature set {set_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
